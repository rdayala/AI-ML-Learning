{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab03A Experiment 01\n",
    "\n",
    "## Naive Bayes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "In this experiment, we will use the Pima Indians Diabetes Data Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Naive Bayes classifier we are finding the accuracies and estimating the probability distribution\n",
    "Following are the concepts that are used in this experiment : \n",
    "1. Calculating prior and posterior probability\n",
    "2. Gaussian Distribution\n",
    "3. Finding likelihood for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Datasets/pima-indians-diabetes.csv\",header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some data samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Pima Indians Diabetes Data Set\\n')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8th column is the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n\\nStats for the 7 features over the dataset and the 2 classes {8th column}{diabetic/not-diabetic}\\n')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data into train and test data, retaining 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIO = 0.8        # 80% training data\n",
    "picker = list(range(data.shape[0]))        # get all indices as a list\n",
    "## sometimes the data is arranged classwise and not randomly\n",
    "## therefore we shuffle the indices\n",
    "random.shuffle(picker)\n",
    "trainMax = int(data.shape[0] * TRAIN_TEST_RATIO)\n",
    "\n",
    "train_features = []\n",
    "test_features = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "'''iterating through the 80% of the samples and appending the last column value in train features \n",
    "and their respective labels as integers in the train_labels'''\n",
    "\n",
    "for pick in picker[:trainMax]:\n",
    "    train_features.append(data.values[pick][:-1])\n",
    "    train_labels.append(int(data.values[pick][-1]))\n",
    "\n",
    "'''iterating through the 20% of the samples and appending the last column value in test features \n",
    "and their respective labels as integers in the test_labels'''\n",
    "for pick in picker[trainMax:]:\n",
    "    test_features.append(data.values[pick][:-1])\n",
    "    test_labels.append(int(data.values[pick][-1]))\n",
    "'''converting the train_features list into array'''\n",
    "train_features = np.array(train_features)\n",
    "'''converting the test_features list into array'''\n",
    "test_features = np.array(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.values[pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printing the shape and length of the test and train arrays created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_features.shape, len(train_labels), test_features.shape, len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Diabetes\n",
    "\n",
    "We have features and labels present in the dataset. Now we need to learn naive bayes classifier for this dataset.\n",
    "\n",
    "$$ P(y|X) = \\frac{P(X|y)*P(y)}{P(X)} \\propto (\\Pi_{i}P(X_{i}|y))*P(y) $$\n",
    "\n",
    "Here, 'y' represents a class, $ X_{i} $ represents the $ i^{th} $ feature and X is the set of all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate Prior $P(Y)$\n",
    "\n",
    "The formula for prior has been taught in class. This is also called the class probability. $P(Y)$ or $P(Y = y)$ is the fraction of the elements present in a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of unique classes & corresponding number of elements belonging to each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes, counts = np.unique(train_labels, return_counts=True)\n",
    "print(classes)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### I assume my classes are from 0 ... N for some N (Here, we have just 2 classes)\n",
    "num_classes = len(classes)\n",
    "num_feats = train_features.shape[1]  #total number of features\n",
    "total_samples = len(train_labels)    #total number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Find the prior probability of each class as the list `prior`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = np.array([ x*1.0/total_samples for x in counts ])\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Posterior Distribution $P(X| y)$ or $\\Pi_{i}P(X_{i}| y)$ from the data\n",
    "\n",
    "This is a little complicated. For each feature $X_{i}$, we assume that they are uncorrelated (Naive Bayes Assumption) and thus we can write,\n",
    "\n",
    "$$ P(X| y) = \\Pi_{i} P(X_{i} | y) $$\n",
    "\n",
    "We wish to estimate $P(X_{i} | y)$ for each $X_{i}$. Thus for samples corresponding to each class label $y$, we need to estimate the probability distribution. Here, however, our features are real values. So we will assume the distribution to be gaussian and estimate the parameters corresponding to each feature using all training samples per class. Thus, we need to calculate mean (say $ m_{y, i} $) and standard deviation (say $ v_{y, i} $) for each class $y$ and each feature $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the mean and variance per feature dimension here \n",
    "### from the training set from samples belonging to each class label.\n",
    "\n",
    "### Hint: Mean is the just the sum of a vector divided by the length of vector\n",
    "### Hint: Variance is the square of standard deviation.\n",
    "\n",
    "means = np.zeros((num_feats, num_classes)) # every feature, for each class\n",
    "stddev = np.zeros((num_feats, num_classes)) # every feature, for each class\n",
    "\n",
    "# For each class\n",
    "for y in classes: # selecting a class 'y'\n",
    "    pts = train_features[np.where( train_labels == y )[0], :]    # get all samples belonging to 'y'\n",
    "    # For each feature\n",
    "    for i in range(num_feats):\n",
    "        #calculating the mean and standard deviation for every feature in every sample\n",
    "        means[i, y] = np.mean(pts[:, i])\n",
    "        stddev[i, y] = np.std(pts[:, i])\n",
    "\n",
    "### This completes the training phase\n",
    "### We know have estimated both the prior probability and the posterior distributions from our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine prior and posterior to classify points.\n",
    "\n",
    "Now, given the mean 'm' and standard deviation 'v', the posterier probability is estimated by the normal distribution (also called gaussian distribution) using:\n",
    "\n",
    "$$ P(X_{i}| y) = \\frac{1}{\\sqrt{2\\pi v_{y, i}^{2}}} exp( - \\frac{ (X_{i} - m_{y, i})^{2} }{2v_{y, i}^{2}} ) $$ \n",
    "\n",
    "\n",
    "Use the formula given above and the parameters computed earlier, we predict the likelihood of every sample in a test set to belong to each class. $\\Pi$ stands for multiplication of each $P(X_{i}|y)$ ($i$ is the $i^{th}$ feature).\n",
    "\n",
    "Note: One may use other kinds of distributions too.\n",
    "\n",
    "### Exercise 2: Complete the Gaussian function based on the above equation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(x, m, v):\n",
    "    g = (1/(np.sqrt(2*3.14*v*v)))*(np.exp( -1.0*(((x - m)/v)**2)*(1/2)))\n",
    "    print(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Find the likelihood for each class 'y', once you have $P(X_{i}|y)$ from Exercise 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_likelihood(point, means, stddev):\n",
    "    \n",
    "    feat_prob = np.zeros((num_feats, num_classes))\n",
    "    for y in classes:\n",
    "        for i in range(num_feats):\n",
    "            feat_prob[i, y] = gaussian(point[i], means[i, y], stddev[i, y]) # get the probability\n",
    "    \n",
    "    likelihood = np.zeros((num_classes, 1)) # likelihood for each class 'y'\n",
    "    for y in classes:\n",
    "    # Take the product of all the feature likelihoods of the class considered\n",
    "        likelihood[y] = np.prod(feat_prob[np.nonzero(feat_prob), y]) # mutliply for each feature 'Xi'\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# For each test sample\n",
    "for i in range(len(test_labels)):\n",
    "    \n",
    "    # Get its likelihood of belong to either class\n",
    "    likelihood = get_likelihood(test_features[i, :], means, stddev)\n",
    "    \n",
    "    # Calculate the approximate posterior = likelihood * prior\n",
    "    approx_posterior = [ np.asscalar(x*y) for x,y in zip(likelihood, prior) ]\n",
    "    #approx because of missing P(X) (constant) in the denominator\n",
    "    \n",
    "    # Make the prediction as that class with the maximum approximate posterior\n",
    "    prediction = np.argmax(approx_posterior)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy\")\n",
    "print(np.mean([x == y for x, y in zip(predictions, test_labels)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4: Experiment with different size of train/test split to see the effect it has on classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just change the TRAIN_TEST_RATIO to different values like 0.6, 0.7 and so on and rerun the code and understand the differences in the accuracies using naive bayes classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
