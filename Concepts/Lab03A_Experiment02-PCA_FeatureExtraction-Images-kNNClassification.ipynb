{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab03A Experiment 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA FEATURE EXTRACTION ##\n",
    "\n",
    "- extract eigenvalues and eigenvectors\n",
    "- choose the best N principal components as features\n",
    "\n",
    "We will use the familiar CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of CIFAR-10 (Lab03_Experiment03)##\n",
    "\n",
    "**data** a 50,000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "**labels** a list of 50,000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "### Read the data, and visualize one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Special function to read special files\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "## Visualize the images in CIFAR-10 Dataset\n",
    "## Here get_data unpickles the CIFAR Dataset and stores the data as 50000*3072 dimension in array X \n",
    "## and labels as 50000*1 dimension in array Y. \n",
    "## Visualize function shows the image corresponding to id number.\n",
    "\n",
    "def get_data(file):\n",
    "    my_dict = unpickle(file)\n",
    "    X = my_dict[b'data']\n",
    "    Y = my_dict[b'labels']\n",
    "    names = np.asarray(my_dict[b'filenames'])\n",
    "    list_class = (unpickle(\"../Datasets/cifar-10/batches.meta\")[b'label_names'])\n",
    "    return X, Y, names,list_class\n",
    "                     \n",
    "\n",
    "def visualize_image(X, Y, names, image_id):\n",
    "    rgb = X[image_id,:]\n",
    "    img = rgb.reshape(3, 32, 32).transpose([1, 2, 0])\n",
    "    plt.imshow(img)\n",
    "    plt.title(names[image_id])\n",
    "    plt.show()\n",
    "\n",
    "# Read images\n",
    "imgs1, labels1, names1, classes1 = get_data(\"../Datasets/cifar-10/data_batch_1\")\n",
    "imgs2, labels2, names2, classes2 = get_data(\"../Datasets/cifar-10/data_batch_2\")\n",
    "imgs3, labels3, names3, classes3 = get_data(\"../Datasets/cifar-10/data_batch_3\")\n",
    "imgs4, labels4, names4, classes4 = get_data(\"../Datasets/cifar-10/data_batch_4\")\n",
    "imgs5, labels5, names5, classes5 = get_data(\"../Datasets/cifar-10/data_batch_5\")\n",
    "\n",
    "#concatenating all the images into imgs_train list \n",
    "imgs_train = np.concatenate((imgs1,imgs2,imgs3,imgs4,imgs5), axis=0)\n",
    "\n",
    "#concatenating all the labels into labels_train list \n",
    "labels_train = np.concatenate((labels1,labels2,labels3,labels4,labels5), axis=0)\n",
    "\n",
    "#concatenating all the names into names_train list \n",
    "names_train = np.concatenate((names1,names2,names3,names4,names5), axis=0)\n",
    "\n",
    "#concatenating all the classes into classes_train list \n",
    "classes_train = np.concatenate((classes1,classes2,classes3,classes4,classes5), axis=0)\n",
    "\n",
    "#reading the images that are used for testing\n",
    "imgs_test, labels_test, names_test, classes_test = get_data(\"../Datasets/cifar-10/test_batch\")\n",
    "\n",
    "# Visualize the 5th image\n",
    "pick = 5\n",
    "print(\"Class =\", classes_train[labels_train[pick]])\n",
    "visualize_image(imgs_train, labels_train, names_train, pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the images to get greyscale images, and subtract the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_train(imgs):\n",
    "    \n",
    "    #convert image array [50000 x 3072] to image [50000 x 32 x 32 x 3]\n",
    "    imgs = imgs.reshape(imgs.shape[0], 3, 32, 32).transpose([0, 2, 3, 1])\n",
    "    \n",
    "    #convert to grayscale image [50000 x 32 x 32]\n",
    "    imgs = np.dot(imgs[...,:3], [0.299, 0.587, 0.114])\n",
    "    \n",
    "    # If we are dealing with training images, compute the mean\n",
    "    mean = np.mean(imgs, axis=0)\n",
    "    \n",
    "    #subtract by mean image\n",
    "    imgs = imgs - mean\n",
    "    \n",
    "    #convert back to image array [50000 x 1024]\n",
    "    print(imgs.shape)\n",
    "    imgs = imgs.reshape(imgs.shape[0], 1024)\n",
    "    print(imgs.shape)\n",
    "    \n",
    "    return imgs, mean\n",
    "\n",
    "def preprocess_test(imgs, mean):\n",
    "    \n",
    "    #convert image array [50000 x 3072] to image [50000 x 32 x 32 x 3]\n",
    "    imgs = imgs.reshape(imgs.shape[0], 3, 32, 32).transpose([0, 2, 3, 1])\n",
    "    \n",
    "    #convert to grayscale image [50000 x 32 x 32]\n",
    "    imgs = np.dot(imgs[...,:3], [0.299, 0.587, 0.114])\n",
    "    \n",
    "    #subtract by mean image calculated from the training set\n",
    "    imgs = imgs - mean\n",
    "    \n",
    "    #convert back to image array [50000 x 1024]\n",
    "    print(imgs.shape)\n",
    "    imgs = imgs.reshape(imgs.shape[0], 1024)\n",
    "    print(imgs.shape)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "imgs_train, mean = preprocess_train(imgs_train)\n",
    "imgs_test = preprocess_test(imgs_test, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First $N$ Principal Components with maximum eigenvalues\n",
    "\n",
    "As we have learnt in the lecture, PCA finds the set of orthonormal vectors which best\n",
    "describe the distribution of the underlying dataset. In the given dataset, we have $n$\n",
    "images of size $K \\times K$. (We know that $K = 32$, and $n = 50000$ in the image set chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Equations related to eigenvalues and eigenvectors\n",
    "\n",
    "The first step towards finding Principal Components is to find the eigenvalues and eigenvectors of the co-variance matrix of our data.\n",
    "\n",
    "From the last subsection, we have the data matrix $\\pmb A$ as an $n \\times K^2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assigning imgs to \"A\"\n",
    "A = imgs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pmb C$, the covariance matrix of $\\pmb A$, is shown in the below equation:\n",
    "\n",
    "$$\n",
    "% C = \\frac{1}{n}\\sum_{i=1}^{n}A^T.A\n",
    "\\pmb C = \\frac{1}{n}\\ (\\pmb A^T.\\pmb A)\n",
    "$$\n",
    "\n",
    "This can be coded in Python as : `C = 1 / A.shape[0] * np.dot(A.T, A)`. Its size is $K^2 \\times K^2$, i.e. the shape of $\\pmb C$ is $(K^2, K^2)$.\n",
    "\n",
    "The eigenvaues and eigenvectors of $\\pmb C$ can be found in Python as: `w, v = np.linalg.eig(C)`, where `w` are the eigenvalues, and `v` is the matrix of eigenvectors or using Singular Value Decomposition: `np.linalg.svd(A,full_matrices=False, compute_uv=True)` \n",
    "\n",
    "Since $\\pmb C$ is a $K^2 \\times K^2$ matrix, there should be $K^2$ eigenvalues, and $K^2$ eigenvectors each of $K^2$ dimensions. So `w` is a numpy array of shape ($K^2$,), i.e. it contains $K^2$ number of eigenvalues. `v` is a numpy array of shape ($K^2$, $K^2$), where each **column** of `v` is an eigenvector. So there are $K^2$ eigenvectors, each of shape ($K^2$,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Problem\n",
    "\n",
    "But, since our images are of size $32\\times32$, i.e. $K = 32$, we shall be finding eigenvalues and eigenvectors of a $1024\\times1024$-sized covariance matrix $C$. Computing $C$ and then $w$ (the eigenvalues) and $V$ (the eigenvectors) is an intractable task and may result in a Memory Error depending on the dataset used. (To be discussed in the next lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Find eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 : Write a function to find the eigenvalues and eigenvectors of the covariance of matrix A without finding C.**\n",
    "\n",
    "The input to this function is a data matrix $\\pmb A$ (i.e. `A = imgs_train`), and the outputs are `w` (eigenvalues) and `V` (eigenvectors) of $\\pmb A^T.\\pmb A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_eigenvalues_and_eigenvectors(A):\n",
    "    # Your code here\n",
    "    #C is the determinant of the matrix\n",
    "    C = 1 / len(A) * np.dot(A.T, A)\n",
    "    \n",
    "    #this will compute eigenvalues and eigen vectors\n",
    "    U, w, VT = np.linalg.svd(C, compute_uv=True)\n",
    "    return w, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function, let us find the eigenvalues and eigenvectors in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calling the above function \n",
    "eigenvalues_train, eigenvectors_train = find_eigenvalues_and_eigenvectors(imgs_train)\n",
    "print(eigenvalues_train.shape)\n",
    "print(eigenvectors_train.shape)\n",
    "eigenvalues_test, eigenvectors_test = find_eigenvalues_and_eigenvectors(imgs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Reordering, normalizing\n",
    "\n",
    "But, since we found the eigenvalues and eigenvectors in a roundabout way, we need to:\n",
    "- reorder them so that they are in descending order of eigenvalues,\n",
    "- normalize the eigenvectors so that their norms are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REORDER\n",
    "\n",
    "# Find the required order of indices to make decreasing order of eigenvalue\n",
    "sort_index = np.argsort(eigenvalues_train)[::-1]\n",
    "\n",
    "# Use the calculated order of indices to reorder eigenvalues and eigenvectors\n",
    "eigenvalues_train = eigenvalues_train[sort_index]\n",
    "eigenvectors_train = eigenvectors_train[:, sort_index]\n",
    "\n",
    "# Find the required order of indices to make decreasing order of eigenvalue\n",
    "sort_index = np.argsort(eigenvalues_test)[::-1]\n",
    "\n",
    "# Use the calculated order of indices to reorder eigenvalues and eigenvectors\n",
    "eigenvalues_test = eigenvalues_test[sort_index]\n",
    "eigenvectors_est = eigenvectors_test[:, sort_index]\n",
    "\n",
    "\n",
    "#see top 3 eigenvalues\n",
    "print(eigenvalues_train[:3])\n",
    "print(eigenvectors_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the eigenvalues are in decreasing order. Let us check the norm of an eigenvector, it should be 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.linalg.norm(eigenvectors_train[:,1]))    #checking if norm is 1\n",
    "# NORMALIZE (no need to normalize when using SVD as it returns normalized eigenvectors)\n",
    "# eigenvectors_train = eigenvectors_train / np.linalg.norm(eigenvectors_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Computing good value for $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given dataset, there are as many eigenvectors as the number of training examples. This can be verified by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigenvectors_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each column is an eigenvector, there are 1024 eigenvectors, each of 1024 dimensions. But usually, a smaller number $N$ of eigenvectors is chosen as a basis to make feature vectors.\n",
    "\n",
    "To decide the on the number $N$, i.e. the number of most important eigenvectors to keep as the basis, the cumulative sum of eigenvalues (assuming they are in decreasing order) divided by the total sum of eigenvalues, vs. the number of eigenvalues considered ($N$) is plotted.\n",
    "\n",
    "This plot shall show the fraction of total variance retained ($r$) vs. the number of eigenvalues considered ($N$). This way, the plot gives a good understanding of the point of diminishing returns, i.e. the point where little variance is retained by retaining additional eigenvalues.\n",
    "\n",
    "This can be understood by the following equation:\n",
    "\n",
    "$$r = \\frac{\\sum_{k=1}^{N}\\lambda_k}{\\sum_{k=1}^{n}\\lambda_k},\\ \\ \\ \\  N << n$$\n",
    "\n",
    "Plotting $r$ vs $N$ shall give a good idea of the impact of varying $N$ on $r$.\n",
    "\n",
    "Let's say we want to retain only 80% of the variance involved. Then we should look for the minimum value of $N$ for which $r > 0.8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 : Edit 1 line of code to calculate r, plot $r$ vs $M$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot r vs M\n",
    "# Values of M to consider: 1, 2,..., n\n",
    "M = np.array(range(1, imgs_train.shape[1] + 1))\n",
    "\n",
    "# Calculate r for all values of M\n",
    "# Your code here\n",
    "# Hint: Look for \"numpy cumulative sum\"\n",
    "r = np.cumsum(eigenvalues_train)/np.sum(eigenvalues_train)\n",
    "\n",
    "# Plot r vs M\n",
    "plt.plot(M, r)\n",
    "# We take only first 1024 eigenvectors because\n",
    "# rest all correspond to eigen value 0\n",
    "plt.xlabel(\"M\", fontsize=20)\n",
    "plt.ylabel(\"r\", fontsize=20)\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that an $M$ value of around 25 (out of 1024) gives an $r$ value of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- **(If it does not, please recheck your code.)**\n",
    "\n",
    "So let us choose $N = 25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we are choosing only $N$ **principal components**. In other words, we are choosing those $N$ types of information that are most important (preserving $80\\%$ of the class variance but using only top 25 eigenvectors out of 1024).\n",
    "\n",
    "Let us note the first N principal components, i.e. the first N eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pca_vectors_train has 25(value of N) eigen vectors from the training dataset \n",
    "pca_vectors_train = eigenvectors_train[:, :N]\n",
    "#pca_vectors_test has 25(value of N) eigen vectors from the testing dataset \n",
    "pca_vectors_test = eigenvectors_test[:, :N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Finding features using first $N$ Principal Components\n",
    "\n",
    "Since we are using the most important eigenvectors as the _basis_ vectors, we need to project the data into these basis components to find the relevant features. We do this by finding the dot product of the data maxtrix and the matrix of the most important eigenvectors.\n",
    "\n",
    "We know that the data is of shape $n \\times K^2$. We also know that the `pca_vectors` matrix is of shape $K^2 \\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Edit 1 line of code to find the pca_features of the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#same like above but to find the pca features instead of eigen vectors\n",
    "pca_features_train = np.dot(imgs_train, pca_vectors_train)\n",
    "pca_features_test = np.dot(imgs_test, pca_vectors_test)\n",
    "\n",
    "print(imgs_train.shape)\n",
    "print(pca_features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we have effectively reduced the dimension of features from 1024 to 380 while preserving 80% of variance in the data. We can see that we have transformed our data from $n \\times K^2$ [50000 x 1024] to $n \\times N$ [50000 x 20] where $N$ is the chosen number of principal components.\n",
    "\n",
    "## 1.7. Visualizing Variance captured by topmost eigenvectors##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the range of values that each eigenvector is able to capture. This shall give us a good idea of the amount of variance each eigenvector is capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_samples =  100\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(141)\n",
    "plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 0], c='r')\n",
    "plt.xlabel('PCA 1st dimension')\n",
    "plt.ylim([-2500, 2500])\n",
    "plt.subplot(142)\n",
    "plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 1], c='y')\n",
    "plt.xlabel('PCA 2nd dimension')\n",
    "plt.ylim([-2500, 2500])\n",
    "plt.subplot(143)\n",
    "plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 3], c='y')\n",
    "plt.xlabel('PCA 3rd dimension')\n",
    "plt.ylim([-2500, 2500])\n",
    "plt.subplot(144)\n",
    "plt.scatter(np.arange(number_of_samples), pca_features_train[:number_of_samples, 4], c='y')\n",
    "plt.xlabel('PCA 4th dimension')\n",
    "plt.ylim([-2500, 2500])\n",
    "plt.title('Visualizing variances captured by eigenvectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the amount of variance captured is decreasing along each eigenvector. Something like this:\n",
    "\n",
    "<img src=\"var.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. k Nearest Neighbours\n",
    "\n",
    "By now, we are quite familiar with the kNN algorithm, so we shall use this to classify test images.\n",
    "\n",
    "**Problem:** Given a test image, classify its label?\n",
    "\n",
    "**Solution:** We shall give the test image's features to a kNN model, and take a majority vote on the k nearest features in the training set.\n",
    "\n",
    "Here is the familiar kNN code (modified slightly to feed train_features and train_labels independently) and also the code for multiclass_classifier done in previous session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "#this function finds the euclidean distance between two points\n",
    "def dist(a, b):\n",
    "    sqSum = 0\n",
    "    for i in range(len(a)):\n",
    "        sqSum += (a[i] - b[i]) ** 2\n",
    "    return math.sqrt(sqSum)\n",
    "\n",
    "\n",
    "def kNN(k, train_features, train_labels, given_feature):\n",
    "    distances = []\n",
    "    for t in range(len(train_features)):\n",
    "        distances.append((dist(train_features[t], given_feature), train_labels[t]))\n",
    "    distances.sort()\n",
    "    #return the sorted K neighbours distances\n",
    "    return distances[:k]\n",
    "\n",
    "def kNN_classify(k, train_features, train_labels, given_feature):\n",
    "    #tally is an empty dictionary initially\n",
    "    tally = collections.Counter()\n",
    "    for nn in kNN(k, train_features, train_labels, given_feature):\n",
    "        #stroing the integer value of the last column in the dataset in the tally\n",
    "        tally.update(str(int(nn[-1])))\n",
    "    #return the top most common among the K neighbours\n",
    "    return int(tally.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's use $k = 3$, and predict the class of the $1^{st}$ image in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kNN_classify(3,  pca_features_train, labels_train, pca_features_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the label of the image is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, kNN has classified this image correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use kNN to classify the first 10 test images using pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels_test = []\n",
    "\n",
    "# Predict labels on the test set\n",
    "for i in range(10):\n",
    "    print(\"Predicting\", i+1, \"of 10\")\n",
    "    predicted_labels_test.append(kNN_classify(3, pca_features_train, labels_train, pca_features_test[i]))\n",
    "\n",
    "# Find accuracy\n",
    "kNN_test_accuracy_pca = np.mean(np.array(predicted_labels_test) == np.array(labels_test[:10]))\n",
    "print(\"Accuracy =\", kNN_test_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, using top 25 pca features, we got 0.3 accuracy on the first 10 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 4: Experiment with different values of N (5-20), and observe the effects on accuracy ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values_of_N = [5, 10, 15, 20, 25]\n",
    "for N in values_of_N:\n",
    "    print(\"N = \" + str(N))\n",
    "    pca_vectors_train = eigenvectors_train[:, :N]\n",
    "    pca_vectors_test = eigenvectors_test[:, :N]\n",
    "\n",
    "    pca_features_train = np.dot(imgs_train, pca_vectors_train)\n",
    "    pca_features_test = np.dot(imgs_test, pca_vectors_test)\n",
    "\n",
    "    print(imgs_train.shape)\n",
    "    print(pca_features_train.shape)\n",
    "    predicted_labels_test = []\n",
    "\n",
    "    # Predict labels on the test set\n",
    "    for i in range(10):\n",
    "        print(\"Predicting\", i+1, \"of 10\")\n",
    "        predicted_labels_test.append(kNN_classify(3, pca_features_train, labels_train, pca_features_test[i]))\n",
    "\n",
    "    # Find accuracy\n",
    "    kNN_test_accuracy_pca = np.mean(np.array(predicted_labels_test) == np.array(labels_test[:10]))\n",
    "    print(\"Accuracy =\", kNN_test_accuracy_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
